<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Machine learning in computational fluid dynamics</title>

		<meta name="description" content="Machine learning in computational fluid dynamics">
		<meta name="author" content="Andre Weiner">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0">

		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
		<link rel="stylesheet" href="plugin/chalkboard/style.css">
		<link rel="stylesheet" href="plugin/customcontrols/style.css">
		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/white.css" id="theme">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>

	<style>
		.reveal .slide-number {
		position: absolute;
		display: block;
		left: 8px;
		bottom: 8px;
		height: 55px;
		width: 100px;
		z-index: 31;
		font-family: Helvetica, sans-serif;
		font-size: 32px;
		line-height: 1;
		color: #000000;
		background-color: rgba(0, 0, 0, 0);
		padding: 5px;
		}
	</style>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<h3>Machine learning in computational fluid dynamics - <span style="color: red;">lectures 14 & 15</span></h3>
					<p>
						Andre Weiner<br>
						<small>TU Braunschweig, <a
								href="https://www.tu-braunschweig.de/en/ism">ISM</a>, Flow Modeling and Control Group</small>
					<small>
						<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a><br />These slides and most of the linked resources are licensed under a<br /> <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.
					</small>
					</p>
	
					<style>
						.row_logo {
							display: table;
						}
	
						.column_logo {
							display: table-cell;
							vertical-align: middle;
							text-align: center;
							width: 50%;
						}
	
						.content_logo {
							display: inline-block;
						}
					</style>
					<div class="row_logo">
						<div class="column_logo">
							<div class="content_logo">
								<img src="images/tubs_logo.png" alt="tubs_logo"
									style="background:none; border:none; box-shadow:none;">
							</div>
						</div>
						<div class="column_logo">
							<div class="content_logo">
								<table>
									<tr>
										<td style="border: 0; padding-right: 5px;">&#9993;</th>
										<td style="border: 0; padding-right: 20px;"><a
												href="mailto:a.weiner@tu-bs.de">Mail</a></th>
										<td style="border: 0; padding-right: 5px;">&#9741;</td>
										<td style="border: 0; padding-right: 0px;"><a
												href="https://www.linkedin.com/in/andre-weiner-a79752133/">LinkedIn</a></td>
									</tr>
									<tr>
										<td style="border: 0; padding-right: 5px;">&#9997;</td>
										<td style="border: 0; padding-right: 20px;"><a
												href="https://ml-cfd.com/">Blog</a></td>
										<td style="border: 0; padding-right: 5px;">&#10026;</td>
										<td style="border: 0; padding-right: 0px;"><a
												href="https://github.com/AndreWeiner">Github</a></td>
									</tr>
								</table>
							</div>
						</div>
					</div>
				</section>

				<section>
					<h2>ProDigi</h2>
					<p>
						This lecture is financed via the program:<br><a href="https://www.tu-braunschweig.de/en/teaching-and-media-education/our-services/international-teaching/prodigi">Promoting digital education through global interconnection</a> (ProDigi).
					</p>
					<p>Let's take a moment to give something back!<br>$\rightarrow$ follow the link to the poll</p>
				</section>

				<section>
					<section>
						<h3>Course exam</h3>
						<ul>
							<li>written exam, ISM lecture room</li>
							<li>March 30, 2023, 10:00 am</li>
						</ul>
						<p>Who plans to take the exam?</p>
					</section>
				</section>
	
				<section>
					<h2>Outline</h2>
					<p>Analyzing coherent structures in flows displaying transonic shock buffets</p>
					<ul>
						<li>transonic shock buffets</li>
						<li>principal component analysis (PCA)</li>
						<li>ways to compute the PCA</li>
						<hr>
						<li>PCA applied to the flow around an airfoil</li>
						<li>dynamic mode decomposition (DMD)</li>
						<li>DMD applied to the flow around an airfoil</li>
					</ul>
				</section>
	
				<section>
					<h2>Outline</h2>
					<p>Controlling the flow past a cylinder</p>
					<ul>
						<li>Flow control</li>
						<li>Elements of reinforcement learning</li>
						<li>Q-learning</li>
						<hr>
						<li>Elements of deep reinforcement learning</li>
						<li>Proximal policy optimization</li>
						<li>Concluding remarks</li>
					</ul>
				</section>

				<section>
					<section>
						<h2>Active flow control</h2>
					</section>
					<section>
						<p>Goals of flow control:</p>
						<ul>
							<li>drag reduction</li>
							<li>load reduction</li>
							<li>process intensification</li>
							<li>noise reduction</li>
							<li>...</li>
						</ul>
					</section>
					<section>
						<p>Categories of flow control:</p>
						<ol>
							<li><b>passive:</b> modification of geometry, topology, fluid, ...</li>
							<li><b>active:</b> flow actuation through moving parts, blowing/suction, heating/cooling, ...</li>
						</ol>
						<p>Active flow control can be more effective but requires <span style="color: red;">energy</span>.</p>
					</section>
					<section>
						<p>Categories of active flow control:</p>
						<ol>
							<li><b>open-loop:</b> actuation prescript; constant or periodic motion, blowing, heating, ...</li>
							<li><b>closed-loop:</b> actuation based on sensor input</li>
						</ol>
						<p>Closed-loop flow control can be more effective but defining the control law is extremely challenging.</p>
					</section>
					<section>
						<video height="500" controls mute autoplay loop>
							<source src="images/cylinder_short.mp4">
						</video>
						<p>Flow past a circular cylinder at $Re=100$.</p>
					</section>
					<section>
						<img src="images/cylinder_drag_lift.png" alt="probes" width="80%" style="background:none; border:none; box-shadow:none;">
						<p>Drag and lift coefficients for the cylinder surface; source: figure 2.9 in <a href="https://doi.org/10.5281/zenodo.4897961">D. Thummar 2021</a>.</p>
					</section>
					<section>
						<p>Typical steps in open-loop control:</p>
						<ol>
							<li>define (parametrized) control law</li>
							<li>optional: optimize control law
								<ul>
									<li>define loss/objective function</li>
									<li>optimize control law iteratively</li>
								</ul>
							</li>
						</ol>
					</section>
					<section>
						<p>Open-loop example:</p>
						<ul>
							<li>periodic cylinder rotation with<br>
								$$\omega = A \mathrm{sin}(2\pi ft) $$
							</li>
							<li>optimization of $A$ and $f$ by searching</li>
							<li>loss<br> <small>$$\Phi = \sqrt{\bar{C_D}^2 + \bar{C_L}^2 + (C_{D_{max}}-C_{D_{min}})^2 +(C_{L_{max}}-C_{L_{min}})^2}$$</small></li>
							<li>$\Omega = d/(2\bar{U}) A$ and $S_f = d/\bar{U}f$</li>
						</ul>
					</section>
					<section>
						<img src="images/cylinder_openloop_loss_surface.png" alt="probes" width="50%" style="background:none; border:none; box-shadow:none;">
						<p>Loss landscape for periodic control; source: figure 3.5 in <a href="https://doi.org/10.5281/zenodo.4897961">D. Thummar 2021</a>.</p>
					</section>
					<section>
						<img src="images/state_action.png" alt="probes" width="50%" style="background:none; border:none; box-shadow:none;">
						<p>Closed-loop example: $\omega = f(\mathbf{p})$.</p>
					</section>
					<section>
						<video height="500" controls mute autoplay loop>
							<source src="images/variable_inflow_2x.mp4">
						</video>
						<p>Closed-loop flow control with variable Reynolds number; source: <a href="https://doi.org/10.5281/zenodo.5634050">F. Gabriel 2021</a>.</p>
					</section>
					<section>
						<p>How to find the control law?</p>
						<ul>
							<li>careful manual design</li>
							<li>adjoint optimization</li>
							<li>machine learning control (MLC)</li>
							<li><b>(deep) reinforcement learning (DRL)</b></li>
						</ul>
					</section>
					<section>
						<p>Favorable attributes of DRL:</p>
						<ul>
							<li>sample efficient thanks to NN-based function approximation</li>
							<li>discrete, continuous, or mixed states and actions</li>
							<li>control law is learnt from scratch</li>
							<li>can deal with uncertainty</li>
							<li>high degree of automation possible</li>
						</ul>
					</section>
					<section>
						<p>Why CFD-based closed-loop control via DRL?</p>
						<ul>
							<li>save virtual environment</li>
							<li>prior optimization, e.g., sensor placement</li>
						</ul>
						<p>Main challenge: CFD environments are expensive!</p>
					</section>
				</section>

				<section>
					<section>
						<h2>Elements of reinforcement learning</h2>
					</section>
					<section>
						<img src="images/reinforcement.svg" alt="probes" width="100%" style="background:none; border:none; box-shadow:none;">
						<p>Reinforcement learning cycle.</p>
					</section>
					<section>
						<p>The agent:</p>
						<ul>
							<li>interacts, evaluates, improves</li>
							<li>acts on the environment following a policy $\pi$</li>
							<li>observes how the state changes</li>
							<li>receives a reward from the environment</li>
							<li>evaluates and improves policy</li>
						</ul>
					</section>
					<section>
						<p>The environment:</p>
						<ul>
							<li>is defined by a state $s$</li>
							<li>transitions from $s$ to $s^\prime$ when acted on</li>
							<li>returns a reward $r$</li>
							<li>defines an extrinsic task expressed by reward</li>
						</ul>
					</section>
					<section>
						<p>States $s\in S$:</p>
						<ul>
							<li>define environment</li>
							<li>can be discrete, continuous, or mixed</li>
							<li>lie in the state space $S$</li>
							<li>are typically not fully available -> observation</li>
							<li>change upon action according to transition function</li>
						</ul>
						<p>Note that state and observation are typically used as synonyms.</p>
					</section>
					<section>
						<p>Actions $ a\in A(s)$:</p>
						<ul>
							<li>are determined by policy $\pi (s)$</li>
							<li>lie in the action space $A(s)$</li>
							<li>may be state-dependent</li>
							<li>can be discrete, continuous, or mixed</li>
						</ul>
					</section>
					<section>
						<p>Even more definitions:</p>
						<ul>
							<li>task: overall goal; episodic or continuing</li>
							<li>time step $t$: one interaction between agent and environment</li>
							<li>episode: cumulation of time steps in episodic tasks</li>
							<li>horizon: time step limit; finite or infinite</li>
							<li>experience tuple: $\{ S_t, A_t, R_{t+1}, S_{t+1}\}$</li>
							<li>trajectory: all experience tuples in an episode</li>
						</ul>
					</section>
					<section>
						<img src="images/slippery_walk_7.png" alt="probes" width="100%" style="background:none; border:none; box-shadow:none;">
						<p>Describing uncertainty: "slippery walk" presented as Markov decision process (MDP). </p>
					</section>
					<section>
						<p>Markov property:</p>
						<p>
							$$
							  P(S_{t+1}|S_t, A_t) = P(S_{t+1}|S_t, A_t, S_{t-1}, A_{t-1}, ...)
							$$
						</p>
						<p>May be relaxed in practice by adding several time levels to the state.</p>
					</section>
					<section>
						<p>Reward function:</p>
						<p>
							$$
							p(s^\prime | s, a) = P(S_{t}=s^\prime | S_{t-1} = s, A_{t-1} = a)
							$$
						</p>
						<p>
							$$
							r(s, a) = \mathbb{E}[R_{t} | S_{t-1}=s, A_{t-1}=a]
							$$
						</p>
						<p>Transition function:</p>
						<p><a href="https://ai.stackexchange.com/questions/21767/why-are-the-value-functions-sometimes-written-with-capital-letters-and-other-tim">What are the upper and lower case letters about?</a></p>
					</section>
					<section>
						<p>Slippery walk in gym/gym_walk:</p>
						<pre class="python"><code>
env = gym.make('SlipperyWalkSeven-v0')
init_state = env.reset()
P = env.env.P
list(P.items())[-2:]
# output
[(7,
  {0: [(0.5000000000000001, 6, 0.0, False),
    (0.3333333333333333, 7, 0.0, False),
    (0.16666666666666666, 8, 1.0, True)],
   1: [(0.5000000000000001, 8, 1.0, True),
    (0.3333333333333333, 7, 0.0, False),
    (0.16666666666666666, 6, 0.0, False)]}),
 (8,
  {0: [(0.5000000000000001, 8, 0.0, True),
    (0.3333333333333333, 8, 0.0, True),
    (0.16666666666666666, 8, 0.0, True)],
   1: [(0.5000000000000001, 8, 0.0, True),
    (0.3333333333333333, 8, 0.0, True),
    (0.16666666666666666, 8, 0.0, True)]})]
						</code></pre>
					</section>
					<section>
						<ul>
							<li>state space: $s\in \{0, 1, 2, 3, 4, 5, 6, 7, 8 \}$</li>
							<li>action space: $a \in \{0, 1\}$</li>
							<li>reward space: $r \in \{0, 1\}$</li>
							<li>experience tuple: $(3, 0, 0, 4)$</li>
							<li>trajectory: $((5, 0, 0, 6), (6, 1, 0, 7), ...)$</li>

						</ul>
					</section>
					<section>
						<p>What is the meaning of $(3, 0, 0, 4)$?</p>
						<ol>
							<li style="list-style-type: upper-latin;">action go right</li>
							<li style="list-style-type: upper-latin;">transition from 4 to 3</li>
							<li style="list-style-type: upper-latin;">reward of 0</li>
						</ol>
					</section>
					<section>
						<p>Why might the CFD environment be uncertain?</p>
						<ol>
							<li style="list-style-type: upper-latin;">random actions</li>
							<li style="list-style-type: upper-latin;">random state changes</li>
							<li style="list-style-type: upper-latin;">incomplete state (observation)</li>
						</ol>
					</section>
					<section>
						<p>Intuitively, what is the optimal policy?</p>
						<ol>
							<li style="list-style-type: upper-latin;">always go left</li>
							<li style="list-style-type: upper-latin;">always go right</li>
							<li style="list-style-type: upper-latin;">alternate left and right</li>
							<li style="list-style-type: upper-latin;">depends on initial state</li>
							<li style="list-style-type: upper-latin;">it doesn't matter - all policies optimal</li>
						</ol>
					</section>
					<section>
						<p>Dealing with sequential feedback:</p>
						<p>return:
							$$
							  G_t = R_{t+1} + R_{t+2} + ... + R_T
							$$
						</p>
						<p>discounted return:
							$$
							  G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... \gamma^{T-1}R_T
							$$
						</p>
						<p>$T$ - number of time steps, $\gamma$ - discounting factor, typically $\gamma = 0.99$</p>
					</section>
					<section>
						<p>recursive definition:
							$$
							  G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... \gamma^{T-1}R_T
							$$
							$$
							  G_{t+1} = R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} + ... \gamma^{T-1}R_T
							$$
							$$
							  G_t = R_{t+1} + \gamma G_{t+1}
							$$
						</p>
					</section>
					<section>
						<p>Why should we discount?</p>
						<p>
							rewards trajectory 1: $(0, 0, 0, 0, 1)$<br>
							rewards trajectory 2: $(0, 0, 0, 0, 0, 0, 1)$<br>
						</p>
						<p>Which one is better?</p>
						<p>returns: 1 and 1</p>
						<p>discounted returns: $0.99^5\cdot 1$ and $0.99^7\cdot 1$</p>
						<p>$\rightarrow$ discounting expresses urgency</p>
					</section>
					<section>
						<p>Returns combined with uncertainty: the state-value function</p>
						<p>
							$$
							  v_\pi (s) = \mathbb{E}_\pi [G_t| S_t=s] = \mathbb{E}_\pi [R_{t+1} + \gamma G_{t+1}| S_t=s]
							$$
							In words: the value function expresses the expected return at time step $t$ given that $S_t = s$ when following policy $\pi$.
						</p>
					</section>
					<section>
						<p>How to compute the value-function if the MDP is known? - Bellman equation</p>
						<p>
							$$
							v_\pi (s) = \mathbb{E}_\pi [R_{t+1} + \gamma G_{t+1}| S_t=s]
							$$
							$$
							v_\pi (s) = \sum\limits_{s^\prime , r, a} p(s^\prime,r| s, a) [r + \gamma \mathbb{E}_\pi [G_{t+1} | S_{t+1} = s^\prime)]]
							$$
							$$
							v_\pi (s) = \sum\limits_{s^\prime , r, a} p(s^\prime,r| s, a) [r + \gamma v_\pi (s^\prime)]\quad \forall s\in S
							$$
						</p>
						<p>$s^\prime$ - next state; deterministic actions assumed</p>
					</section>
					<section>
						<p>Intuitively, which state has the highest value?</p>
						<ol>
							<li style="list-style-type: upper-latin;">0</li>
							<li style="list-style-type: upper-latin;">3</li>
							<li style="list-style-type: upper-latin;">5</li>
							<li style="list-style-type: upper-latin;">6</li>
							<li style="list-style-type: upper-latin;">all equal</li>
						</ol>
					</section>
					<section>
						<p>What action to take? The state-action function:</p>
						<p>
							$$
							q_\pi (s, a) = \mathbb{E}_\pi [G_t | S_t = s, A_t=a]
							$$
							$$
							q_\pi (s, a) = \mathbb{E}_\pi [R_{t+1} + \gamma G_{t+1} | S_t = s, A_t=a]
							$$
							$$
							q_\pi (s, a) = \sum\limits_{s^\prime , r} p(s^\prime,r| s, a)[r + \gamma v_\pi (s^\prime)]\quad \forall s\in S, \forall a\in A
							$$
						</p>
					</section>
					<section>
						<p>Intuitively, what would you expect $q_\pi (s, a=0)$ vs. $q_\pi (s, a=1)$ ?</p>
						<ol>
							<li style="list-style-type: upper-latin;"> $q_\pi (s, a=0) > q_\pi (s, a=1)$</li>
							<li style="list-style-type: upper-latin;"> $q_\pi (s, a=0) = q_\pi (s, a=1)$</li>
							<li style="list-style-type: upper-latin;"> $q_\pi (s, a=0) < q_\pi (s, a=1)$</li>
						</ol>
					</section>
					<section>
						<p>How much better is an action? - action-advantage function</p>
						<p>
							$$
							a_\pi (s,a) = q_\pi(s,a) - v_\pi(s)
							$$
						</p>
					</section>
					<section>
						<p>The optimal policy:</p>
						<p>
							$$
							v_\ast (s) = \underset{\pi}{\mathrm{max}}\ v_\pi (s)\quad \forall s\in S
							$$
							$$
							q_\ast (s, a) = \underset{\pi}{\mathrm{max}}\ q_\pi (s, a)\quad \forall s\in S, \forall a\in A
							$$
						</p>
					</section>
					<section>
						<p>Planning: finding $\pi_\ast$ with value iterations</p>
						<p>
							$$
							v_{k+1}(s) = \underset{a}{\mathrm{max}}\sum\limits_{s^\prime, r} p(s^\prime , r | s, a)[r+\gamma v_k(s^\prime)]
							$$
						</p>
						<p>$v_k$ - value estimate at iteration $k$</p>
					</section>
					<section>
						<p>Value iteration implementation:</p>
						<pre class="python"><code>
def value_iteration(P: Dict[int, Dict[int, tuple]], gamma: float=0.99, theta: float=1e-10):
    V = pt.zeros(len(P))
    while True:
        Q = pt.zeros((len(P), len(P[0])))
        for s in range(len(P)):
            for a in range(len(P[s])):
                for prob, next_state, reward, done in P[s][a]:
                    Q[s][a] += prob * (reward + gamma * V[next_state] * (not done))
        if pt.max(pt.abs(V - pt.max(Q, dim=1).values)) < theta:
            break
        V = pt.max(Q, dim=1).values
    pi = lambda s: {s:a for s, a in enumerate(pt.argmax(Q, dim=1))}[s]
    return Q, V, pi
						</code></pre>
					</section>
					<section>
						<p>Optimal action-value function:</p>
						<pre class="python"><code>
optimal_Q, optimal_V, optimal_pi = value_iteration(P)
optimal_Q[1:-1]
# output
tensor([[0.3704, 0.6668],
        [0.7902, 0.8890],
        [0.9302, 0.9631],
        [0.9768, 0.9878],
        [0.9924, 0.9960],
        [0.9976, 0.9988],
        [0.9993, 0.9997]])
						</code></pre>
					</section>
					<section>
						<p>Why is it called reinforcement <b>learning</b> and not <b>planning</b>?</p>
						<ol>
							<li style="list-style-type: upper-latin;">both are equivalent</li>
							<li style="list-style-type: upper-latin;">reward function unknown</li>
							<li style="list-style-type: upper-latin;">transition function unknown</li>
							<li style="list-style-type: upper-latin;">reward and transition function unknown</li>
						</ol>
					</section>
				</section>

				<section>
					<section>
						<h2>Q-learning</h2>
					</section>
					<section>
						<p>What to do if the MDP is partially unknown?</p>
						<p>trail and error learning</p>
						<p>$\rightarrow$ requires mix between exploration and exploitation</p>
					</section>
					<section>
						<p>General learning strategy:</p>
						<ol>
							<li>initialize random Q function</li>
							<li>sample next action</li>
							<li>interact with environment</li>
							<li>update value and/or action-value estimate</li>
							<li>update policy</li>
							<li>go back to 2. until convergence</li>
						</ol>
					</section>
					<section>
						<p>Example action-value function table:</p>
						<pre class="python"><code>
# slippery walk Q-table
{...
 7: [0.1, 0.9], # greedy action: 1
 8: [0.2, 0.1]} # greedy action: 0
						</code></pre>
					</section>
					<section>
						<p>Exploration vs. exploitation strategies:</p>
						<ul>
							<li>act always greedily</li>
							<li>act always random</li>
							<li>act $\epsilon$-greedily</li>
							<li>decaying $\epsilon$-greedy</li>
							<li>Thomson sampling</li>
						</ul>
					</section>
					<section>
						<p>SARSA and Q-learning update rules</p>
						<p>
							<small>
							$$
							  Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha_t \left[ R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \right]
							$$
							$$
							  Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha_t \left[ R_{t+1} + \gamma \underset{a}{\mathrm{max}}(Q(S_{t+1}, a)) - Q(S_t, A_t) \right]
							$$
						</small>
						</p>
						<p>$\alpha_t$ - learning rate, typically decaying</p>
						<p>$Q$-learning is an <b>offline</b> learning strategy.</p>
					</section>
					<section>
						<p>Limitations of tabular learning methods:</p>
						<ul>
							<li>high-dimensional state/action spaces</li>
							<li>continuous state/action spaces</li>
						</ul>
						<p>Solution: RL with neural networks for function approximation - <b>deep</b> RL</p>
					</section>
				</section>

				<section>
					<section>
						<h2>Elements of <span style="color: red;">deep</span> reinforcement learning</h2>
					</section>
					<section>
						<p>Deep RL uses neural networks to lean function approximations of:</p>
						<ul>
							<li>the action-value function</li>
							<li>the value function</li>
							<li>the policy</li>
							<li>the environment</li>
							<li>combinations of all points above</li>
						</ul>
					</section>
					<section>
						<p>Some DRL lingo:</p>
						<ul>
							<li>Value-based methods: approximation of action-value function; NFQ, DQN, DDQN</li>
							<li>Policy-gradient methods: approximation of policy; REINFORCE, VPG</li>
							<li>Model-based methods: approximation of environment</li>
							<li>Actor-critic methods: approximation of policy and value function with bootstrapping; A2C, PPO</li>
						</ul>
					</section>
					<section>
						<p>General learning strategy:</p>
						<ol>
							<li>initialize random policy/action-value functions</li>
							<li>sample trajectory/trajectories from one or more environments</li>
							<li>update network(s)</li>
							<li>go back to 2. until convergence</li>
						</ol>
					</section>
					<section>
						<p>How do DRL agents explore?</p>
						<ul>
							<li>discrete actions: equivalent to tabular methods</li>
							<li>continuous actions:
								<ul>
									<li>add noise to action</li>
									<li>sample action from PDF</li>
								</ul>
							</li>
						</ul>
						<p>PDF - probability density function</p>
					</section>
				</section>

				<section>
					<section>
						<h2>Proximal policy optimization</h2>
					</section>
					<section>
						<img src="images/state_action.png" alt="probes" width="50%" style="background:none; border:none; box-shadow:none;">
						<p>Closed-loop example: $\omega = f(\mathbf{p})$.</p>
					</section>
					<section>
						<img src="images/ppo_overview.png" alt="probes" width="45%" style="background:none; border:none; box-shadow:none;">
						<p>PPO overview; source: figure 4.6 in <a href="https://doi.org/10.5281/zenodo.4897961">D. Thummar 2021</a>.</p>
					</section>
					<section>
						<img src="images/policy_network.png" alt="probes" width="50%" style="background:none; border:none; box-shadow:none;">
						<p>Policy network; source: figure 4.4 in <a href="https://doi.org/10.5281/zenodo.4897961">D. Thummar 2021</a>.</p>
					</section>
					<section>
						<img src="images/exploration.png" alt="probes" width="80%" style="background:none; border:none; box-shadow:none;">
						<p>Action of example trajectories in training mode; source: figure 5.2 in <a href="https://doi.org/10.5281/zenodo.5634050">F. Gabriel 2021</a>.</p>
					</section>
					<section>
						<img src="images/gauss_vs_beta_dist.svg" alt="probes" width="80%" style="background:none; border:none; box-shadow:none;">
						<p>Comparison of Gauss and Beta distribution.</p>
					</section>
					<section>
						<p>learning what to expect in a given state - value function loss</p>
						<p>
							$$
							  L_V = \frac{1}{N_\tau N_t} \sum\limits_{\tau = 1}^{N_\tau}\sum\limits_{t = 1}^{N_t} \left( V(s_t^\tau) - G_t^\tau \right)^2
							$$
						</p>
						<ul>
							<li>$\tau$ - trajectory (single simulation)</li>
							<li>$s_t$ - state/observation (pressure)</li>
							<li>$V$ - parametrized value function</li>
							<li>clipping not included</li>
						</ul>
					</section>
					<section>
						<p>Was the selected action a good one?</p>
						<p>
							$$\delta_t = R_t + \gamma V(s_{t+1}) - V(s_t) $$
							$$
							  A_t^{GAE} = \sum\limits_{l=0}^{N_t-t} (\gamma \lambda)^l \delta_{t+l}
							$$
						</p>
						<ul>
							<li>$\delta_t$ - one-step advantage estimate</li>
							<li>$A_t^{GAE}$ - generalized advantage estimate</li>
							<li>$\lambda$ - smoothing parameter</li>
						</ul>
					</section>
					<section>
						<p>make good actions more likely - policy objective function</p>
						<p><small>
							$$
							  J_\pi = \frac{1}{N_\tau N_t} \sum\limits_{\tau = 1}^{N_\tau}\sum\limits_{t = 1}^{N_t} \mathrm{min}\left[ \frac{\pi(a_t|s_t)}{\pi^{old}(a_t|s_t)} A^{GAE,\tau}_t,
							  \mathrm{clamp}\left(\frac{\pi(a_t|s_t)}{\pi^{old}(a_t|s_t)}, 1-\epsilon, 1+\epsilon\right) A^{GAE,\tau}_t\right]
							$$
						</small>
						</p>
						<ul>
							<li>$\pi$ - current policy</li>
							<li>$\pi^{old}$ - old policy (previous episode)</li>
							<li>entropy not included</li>
							<li>$J_\pi$ is <b>maximized</b></li>
						</ul>
					</section>
					<section>
						<img src="images/ppo_policy_loss_clipping.svg" alt="probes" width="100%" style="background:none; border:none; box-shadow:none;">
						<p>Effect of clipping in the PPO policy loss function.</p>
					</section>
					<section>
						<h3>Why PPO?</h3>
						<ul>
							<li>continuous and discrete actions spaces</li>
							<li><b>relatively</b> simple implementation</li>
							<li>restricted (robust) policy updates</li>
							<li>sample efficient</li>
							<li>...</li>
						</ul>
						<p>Refer to <a href="https://arxiv.org/abs/2006.11005">R. Paris et al. 2021</a> and the references therein for similar works employing PPO.</p>
					</section>
					<section>
						<img src="images/distributions.png" alt="probes" width="80%" style="background:none; border:none; box-shadow:none;">
						<p>Cumulative reward over the number of training episodes; source: figure 5.8 in <a href="https://doi.org/10.5281/zenodo.5634050">F. Gabriel 2021</a>.</p>
					</section>
					<section>
						<video height="500" controls mute autoplay loop>
							<source src="images/variable_inflow_2x.mp4">
						</video>
						<p>Closed-loop flow control with variable Reynolds number; source: <a href="https://doi.org/10.5281/zenodo.5634050">F. Gabriel 2021</a>.</p>
					</section>
					<section>
						<img src="images/transient_drag.png" alt="probes" width="80%" style="background:none; border:none; box-shadow:none;">
						<p>Drag coefficient with and without control; source: figure 6.5 in <a href="https://doi.org/10.5281/zenodo.5634050">F. Gabriel 2021</a>.</p>
					</section>
					<section>
						<img src="images/cylinder_surface_pressure.png" alt="probes" width="45%" style="background:none; border:none; box-shadow:none;">
						<p>Pressure on the cylinder's surface; source: figure 5.7 in <a href="https://doi.org/10.5281/zenodo.5634050">F. Gabriel 2021</a>.</p>
					</section>
				</section>
				<section>
					<section>
						<h2>Concluding remarks</h2>
					</section>
					<section>
						<p>ML can be a value asset in CFD:</p>
						<ul>
							<li>ML creates useful representations of data</li>
							<li>ML helps to find patterns in complex data</li>
							<li>potential for improving speed, accuracy, and understanding</li>
						</ul>
					</section>
					<section>
						<p>Topics we dealt with:</p>
						<ul>
							<li>introduction and CFD and ML</li>
							<li>tackling the high-Schmidt number problem</li>
							<li>predicting the stability regime of rising bubbles</li>
							<li>analyzing coherent structures in transonic buffets</li>
							<li>creating reduced-order models</li>
							<li>closed-loop active flow control</li>
						</ul>
					</section>
					<section>
						<p>Designing good features is essential:</p>
						<ul>
							<li>use physical laws/intuition</li>
							<li>be creative and automate the extraction/selection</li>
							<li>features must be available in target application!</li>
						</ul>
					</section>
					<section>
						<p>Solving the ML problem:</p>
						<ul>
							<li>keep it simple</li>
							<li>learn the mapping that you really need</li>
							<li>create an automated training/validation pipeline</li>
							<li>always look at multiple error metrics</li>
							<li>treat non-linear optimization as an experiment</li>
							<li>combine multiple models instead of creating a master model</li>
						</ul>
						<p>Most often, there is a zoo of possible algorithms, so don't be too focused on one particular algorithm.</p>
					</section>
					<section>
						<p>Don't throw ML on every problem blindly!</p>
						<p>$\rightarrow$ stick to established methods if possible</p>
					</section>
				</section>
				<section>
					<h2>Thank you for attending this course!</h2>
					<p>Any feedback to improve it is welcome!</p>
				</section>


			</div>

		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/zoom/zoom.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/search/search.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/math/math.js"></script>
		<script src="plugin/chalkboard/plugin.js"></script>
		<script src="plugin/customcontrols/plugin.js"></script>
		<script>

			// Also available as an ES module, see:
			// https://revealjs.com/initialization/
			Reveal.initialize({
				controls: true,
				progress: true,
				center: true,
				hash: true,
				chalkboard: {
				boardmarkerWidth: 4,
				chalkWidth: 4,
				chalkEffect: 0.01,
				theme: "chalkboard",
				background: ['rgba(127,127,127,.1)', path + 'img/blackboard.png'],
				grid: { color: 'rgb(50,50,10,0.5)', distance: 80, width: 2 },
				eraser: { src: path + 'img/sponge.png', radius: 20 }
			},
			customcontrols: {
				controls: [
					{
						icon: '<i class="fa fa-pencil-square" style="font-size:36px"></i>',
						title: 'Toggle chalkboard (B)',
						action: 'RevealChalkboard.toggleChalkboard();'
					},
					{
						icon: '<i class="fa fa-pencil" style="font-size:36px"></i>',
						title: 'Toggle notes canvas (C)',
						action: 'RevealChalkboard.toggleNotesCanvas();'
					}
				]
			},

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealZoom, RevealNotes, RevealSearch, RevealMarkdown, RevealHighlight, RevealMath, RevealChalkboard, RevealCustomControls ]
			});
			Reveal.configure({ slideNumber: true });

		</script>

	</body>
</html>
