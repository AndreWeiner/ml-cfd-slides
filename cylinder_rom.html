<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Machine learning in computational fluid dynamics</title>

		<meta name="description" content="Machine learning in computational fluid dynamics">
		<meta name="author" content="Andre Weiner">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0">

		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
		<link rel="stylesheet" href="plugin/chalkboard/style.css">
		<link rel="stylesheet" href="plugin/customcontrols/style.css">
		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/black.css" id="theme">
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<h3><span style="color: red;">Reduced-order modeling for flow fields</span></h3>
					<p>
						Andre Weiner<br>
						<small>TU Dresden, Institute of fluid mechanics, <a href="https://tu-dresden.de/ing/maschinenwesen/ism/psm">PSM</a></small>
						<small>
							<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img
									alt="Creative Commons License" style="border-width:0"
									src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a><br />These slides and most
							of the linked resources are licensed under a<br /> <a rel="license"
								href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0
								International License</a>.
						</small>
					</p>
	
					<style>
						.row_logo {
							display: table;
						}
	
						.column_logo {
							display: table-cell;
							vertical-align: middle;
							text-align: center;
							width: 50%;
						}
	
						.content_logo {
							display: inline-block;
						}
					</style>
					<div class="row_logo">
						<div class="column_logo">
							<div class="content_logo">
								<img src="images/logos/TUD_white.png" alt="tud_logo"
									style="background:none; border:none; box-shadow:none;">
							</div>
						</div>
						<div class="column_logo">
							<div class="content_logo">
								<table>
									<tr>
										<td style="border: 0; padding-right: 5px;">&#9993;</th>
										<td style="border: 0; padding-right: 20px;"><a
												href="mailto:andre.weiner@tu-dresden.de">Mail</a></th>
										<td style="border: 0; padding-right: 5px;">&#9741;</td>
										<td style="border: 0; padding-right: 0px;"><a
												href="https://www.linkedin.com/in/andre-weiner-a79752133/">LinkedIn</a></td>
									</tr>
									<tr>
										<td style="border: 0; padding-right: 5px;">&#9997;</td>
										<td style="border: 0; padding-right: 20px;"><a href="https://ml-cfd.com/">Blog</a>
										</td>
										<td style="border: 0; padding-right: 5px;">&#10026;</td>
										<td style="border: 0; padding-right: 0px;"><a
												href="https://github.com/AndreWeiner">Github</a></td>
									</tr>
								</table>
							</div>
						</div>
					</div>
				</section>

				<section>
					<h2>Last lecture(s)</h2>
					<p>Analyzing coherent structures in flows displaying transonic shock buffets</p>
					<ul>
						<li>transonic shock buffet</li>
						<li>principal component analysis (PCA)</li>
						<li>ways to compute the PCA</li>
						<hr>
						<li>dynamic mode decomposition (DMD)</li>
					</ul>
				</section>
	
				<section>
					<h2>Outline</h2>
					<p>Reduced-order modeling of flow fields</p>
					<ul>
						<li>reduced-order models for dynamical systems</li>
						<li>optimized DMD</li>
						<li>dealing with high-dimensional data</li>
					</ul>
				</section>

				<section>
					<section>
						<h2>Reduced-order models for dynamical systems</h2>
					</section>
					<section>
						<p>What is a reduced-order model (ROM)?</p>
						<ul>
							<li>mathematical model with reduced
								<ul>
									<li>computational complexity</li>
									<li>dimensionality</li>
								</ul>
							</li>
							<li>approximation
							</li>
							<li>often derived from data</li>
						</ul>
					</section>
					<section>
						<p>ROMs for dynamical systems</p>
						<p>
							$$
								\frac{\mathrm{d}\mathbf{x}}{\mathrm{d}t} = F(\mathbf{x}(t), t, ...)
							$$
						</p>
						<p>Definition of ROMs in this lecture:</p>
						<ol>
							<li>encoding: $\tilde{\mathbf{x}}_n = E(\mathbf{x}_n)$</li>
							<li>evolution: $\tilde{\mathbf{x}}_{n+1} \approx f(\tilde{\mathbf{x}}_n)$</li>
							<li>decoding: $\hat{\mathbf{x}}_n \approx E^{-1}(\tilde{\mathbf{x}}_n)$</li>
						</ol>
						<p>$\mathbf{x}$ - full state, $\tilde{\mathbf{x}}$ - reduced state, $\hat{\mathbf{x}}$ - full state prediction</p>
					</section>
					<section>
						<p>Data-driven ROMs:<br> ML for encoding/decoding and/or temporal evolution.</p>
						<p>Criteria for good ROMs:</p>
						<ul>
							<li>computationally efficient</li>
							<li>interpretable states and dynamics</li>
							<li>long-term stability</li>
						</ul>
					</section>
				</section>
			
				<section>
					<section>
						<h2>Optimized DMD</h2>
					</section>
					<section>
						<p>review of some DMD basics</p>
						<ol>
							<li class="fragment">$\mathbf{x}_n$: state vector at time $t_n = n\Delta t$</li>
							<li class="fragment">core idea: $ \mathbf{x}_{n+1}=\mathbf{A}\mathbf{x}_n $</li>
							<li class="fragment">diagonalization: $\mathbf{A}=\mathbf{\Phi}\mathbf{\Lambda}\mathbf{\Phi}^{-1}$</li>
							<li class="fragment">prediction: $\mathbf{x}_{n} = \mathbf{\Phi\Lambda}^{n-1}\mathbf{\Phi}^{-1}\mathbf{x}_1$</li>
						</ol>
						<p class="fragment" style="color: red;">$\rightarrow$ once $\mathbf{A}$ is known, the state at any $t_n$ may be predicted based on a given initial state $\mathbf{x}_1$</p>
					</section>
					<section>
						<p>reconstruction of the full dataset</p>
						<small>
							<p>
	$$
	\underbrace{
		\begin{bmatrix}
		x_{11} & \ldots & x_{1N}\\
		\vdots & \ddots & \vdots\\
		x_{M1} & \ldots & x_{MN}
		\end{bmatrix}
		}_{\mathbf{M}}
	\approx
	\underbrace{
	\begin{bmatrix}
	\phi_{11} & \ldots & \phi_{1r}\\
	\vdots & \ddots & \vdots\\
	\phi_{M1} & \ldots & \phi_{Mr}
	\end{bmatrix}
	}_{\mathbf{\Phi}}
	\underbrace{
	\begin{bmatrix}
	b_1& & \\
	 & \ddots & \\
	 &  & b_r
	\end{bmatrix}
	}_{\mathbf{D_b}}
	\underbrace{
	\begin{bmatrix}
	\lambda_{1}^0 & \ldots & \lambda_{1}^{N-1}\\
	\vdots &  & \vdots\\
	\lambda_r^0 & \ldots & \lambda_r^{N-1}
	\end{bmatrix}
	}_{\mathbf{V_\lambda}}
	$$
							</p>
						</small>
						<p>with $\mathbf{b} = \mathbf{\Phi}^{-1}\mathbf{x}_1$, $M$ - length of $\mathbf{x}$, $N$ - number of snapshots, $r$ - truncation rank</p>
					</section>
					<section>
						<div class="r-hstack justify-around">
							<div>
								<p style="color: rgb(3, 154, 255)">regular DMD</p>
								$\mathbf{X} = \left[ \mathbf{x}_1, \ldots, \mathbf{x}_{N-1} \right]^T$<br>
								$\mathbf{Y} = \left[ \mathbf{x}_2, \ldots, \mathbf{x}_{N} \right]^T$
								<p>
									$$
									\underset{\mathbf{A}}{\mathrm{argmin}}\left|\left| \mathbf{Y}-\mathbf{AX} \right|\right|_F
									$$
								</p>
							</div>
							<div class="fragment">
								<p style="color: rgb(4, 252, 4)">optimizedDMD</p>
								$\mathbf{\Phi_b}=\mathbf{\Phi}\mathbf{D_b}$<br>
								$\mathbf{M} = \left[ \mathbf{x}_1, \ldots, \mathbf{x}_{N} \right]^T$
								<p>
									$$
									\underset{\mathbf{\lambda},\mathbf{\Phi}_\mathbf{b}}{\mathrm{argmin}}\left|\left| \mathbf{M}-\mathbf{\Phi}_\mathbf{b}\mathbf{V}_{\mathbf{\lambda}} \right|\right|_F
									$$
								</p>
							</div>
						</div>
						<p class="fragment" style="color: red;">$\rightarrow$ "optDMD" problem is non-linear and non-convex</p>
					</section>
					<section>
						<p>idea: borrow techniques from ML/DL</p>
						<ul>
							<li>stochastic gradient descent</li>
							<li>automatic differentiation</li>
							<li>train-validation-split</li>
							<li>early stopping</li>
						</ul>
						<p style="color: red;">$\rightarrow$ refer to article for details</p>
					</section>
					<section>
						<img src="images/rom/cylinder_data_noisy.png" alt="sh_glob" width="65%" style="background:none; border:none; box-shadow:none;">
							<p>
								Test data with noise corruption.
							</p>
					</section>
					<section>
						<img src="images/rom/cylinder_rec_norm.png" alt="sh_glob" width="90%" style="background:none; border:none; box-shadow:none;">
							<p>
								Comparison of DMD reconstruction error.
							</p>
					</section>
					<section>
						<img src="images/rom/cylinder_eigvals.png" alt="sh_glob" width="65%" style="background:none; border:none; box-shadow:none;">
							<p>
								Comparison of dominant eigenvalues; reference computed with clean data.
							</p>
					</section>
					<section>
						<img src="images/rom/train_val_loss.png" alt="sh_glob" width="100%" style="background:none; border:none; box-shadow:none;">
							<p>
								Early stopping prevents overfitting.
							</p>
					</section>
					<section>
						<p>optimized DMD with sparsity promotion</p>
						<p>
							$$
							\underset{\mathbf{\lambda},\mathbf{\Phi}_\mathbf{b}}{\mathrm{argmin}}\left(\left|\left| \mathbf{M}-\mathbf{\Phi}_\mathbf{b}\mathbf{V}_{\mathbf{\lambda}} \right|\right|_F + \gamma_0 ||\mathbf{b}||_2\right)
							$$
						</p>
						<p style="color: red;">$\rightarrow$ reduction to dominant dynamics</p>
					</section>
					<section>
						<img src="images/rom/sparsity.png" alt="sh_glob" width="100%" style="background:none; border:none; box-shadow:none;">
							<p>
								Cardinality of modes amplitudes and reconstruction error for increasing sparsity weight $\gamma_0$.
							</p>
					</section>
				</section>

				<section>
					<section>
						<h2>dealing with high-dimensional data</h2>
					</section>
					<section>
						<p>most common strategies</p>
						<ol>
							<li>project data on POD modes</li>
							<li>(<a href="https://stats.stackexchange.com/questions/324340/when-should-i-use-a-variational-autoencoder-as-opposed-to-an-autoencoder">variational</a>) autoencoder</li>
						</ol>
					</section>
					<section>
						<img src="images/rom/cnn_var_ae.png" alt="stencil"
						style="background:white; border:none; box-shadow:none; width: 100%">
					<p>
						CNN-VAR-AE + LSTM; figure 2 from <a href="https://link.springer.com/article/10.1007/s13272-023-00641-6">Zahn et al. (2023).</a>
					</p>
					</section>

				</section>
<!--
				<section>
					<section>
						<h2>Grouping similar data points</h2>
					</section>
					<section>
						<img src="images/clustering_test_data.svg" alt="probes" width="75%" style="background:none; border:none; box-shadow:none;">
						<p>Test data for clustering.</p>
					</section>
					<section>
						<img src="images/clustering_test_data_no_labels.svg" alt="probes" width="75%" style="background:none; border:none; box-shadow:none;">
						<p>Test data for clustering without labels.</p>
					</section>
					<section>
						<img src="images/random_initial_centroids.svg" alt="probes" width="75%" style="background:none; border:none; box-shadow:none;">
						<p>Randomly chosen initial centroids.</p>
					</section>
					<section>
						<img src="images/intial_cluster_labels.svg" alt="probes" width="75%" style="background:none; border:none; box-shadow:none;">
						<p>Randomly chosen initial centroids and cluster association.</p>
					</section>
					<section>
						<img src="images/updated_cluster_labels.svg" alt="probes" width="75%" style="background:none; border:none; box-shadow:none;">
						<p>K-means update of centroids.</p>
					</section>
					<section>
						<img src="images/kmeans_iterations.svg" alt="probes" width="100%" style="background:none; border:none; box-shadow:none;">
						<p>Consecutive iterations of the k-means algorithm.</p>
					</section>
					<section>
						<p>Recap of k-means steps:</p>
						<ol>
							<li>randomly select $k$ data points at initial centroids</li>
							<li>compute the distance between data points and centroids</li>
							<li>select the cluster id for each data point based on the closest centroid</li>
							<li>update each centroid based on the mean of all data points in the cluster</li>
							<li>repeat steps 2-4 until a stopping criterion is reached</li>
						</ol>
					</section>
					<section>
						<img src="images/bad_intial_centroids.svg" alt="probes" width="75%" style="background:none; border:none; box-shadow:none;">
						<p>Unfavorable selection of initial centroids.</p>
					</section>
					<section>
						<img src="images/bad_intial_centroids_final.svg" alt="probes" width="75%" style="background:none; border:none; box-shadow:none;">
						<p>Final centroids and cluster association with bad initialization.</p>
					</section>
					<section>
						<p>Measuring the quality of the clustering - inertia:</p>
						<p>
							$$
							  I = \sum\limits_{i=1}^N \underset{\mu_j}{\mathrm{min}}\left(||\mathbf{x}_i-\mu_j||^2\right)
							$$
						</p>
						<p>$N$ - number of data points, $\mu_j$ - centroid/mean of all data points in cluster $j$</p>
					</section>
					<section>
						<p>Influence of bad initialization on inertia:</p>
						<pre class="python"><code>
def compute_cluster_inertia(centroids: pt.Tensor, data: pt.Tensor) -> float:
    """Compute sum of squared distances over all clusters.
    """
    labels = find_nearest_centroid(centroids, data)
    inertia = 0.0
    for i in range(data.shape[1]):
        inertia += pt.linalg.norm(data[labels==i]-centroids[i], dim=1).square().sum()
    return inertia
# test
# Inertia for random initialization: 110.1418
# Inertia for bad initialization: 540.7017
						</code></pre>
					</section>
					<section>
						<p>Improved initialization - k-means++:</p>
						<ol>
							<li>select first centroid randomly</li>
							<li>compute distance between data points and centroid(s)</li>
							<li>select the minimum distance for each data point</li>
							<li>chose the next centroid randomly with probability proportional to the minimum squared distance</li>
							<li>repeat steps 2-4 until $k$ centroids are chosen</li>
						</ol>
					</section>
					<section>
						<img src="images/improved_initial_centroids.svg" alt="probes" width="75%" style="background:none; border:none; box-shadow:none;">
						<p>K-means++ - improved centroid initialization.</p>
					</section>
					<section>
						<img src="images/random_vs_improved.svg" alt="probes" width="75%" style="background:none; border:none; box-shadow:none;">
						<p>Comparison of random and k-means++ initialization over 100 runs.</p>
					</section>
				</section>

				<section>
					<section>
						<h2>Modeling the transition between clusters</h2>
					</section>
					<section>
						<img src="images/periodic_dataset_raw_time.svg" alt="probes" width="75%" style="background:none; border:none; box-shadow:none;">
						<p>Time series of data (state) with two features.</p>
					</section>
					<section>
						<img src="images/periodic_dataset_clustering.svg" alt="probes" width="50%" style="background:none; border:none; box-shadow:none;">
						<p>Clustering of time series data using 10 centroids.</p>
					</section>
					<section>
						<p>Modeling of the transition time:</p>
						<ol>
							<li>determine sequence of visited centroids</li>
							<li>count consecutive occurrence</li>
							<li>remove consecutive duplicates</li>
							<li>compute residence time</li>
							<li>compute transition time as average between residence times</li>
						</ol>
						<p>assumption: constant time step</p>
					</section>
					<section>
						<p>Transition time example:</p>
						<pre class="python"><code>
transition_times = compute_transition_time(cluster_ids, 2.0*np.pi/100, True)
# output
# sequence:  [5 5 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 8 8 ...]
# centroid sequence:  [5 2 4 8 0 9 7 3 6 1 5]
# sequential duplicates:  [ 2  9 10  8  9  9 10 10 11 11 11]
transition_times
#{'5,2': 0.3455751918948773,
# '2,4': 0.5969026041820608,
# '4,8': 0.5654866776461628,
# '8,0': 0.5340707511102649,
# '0,9': 0.5654866776461628,
# '9,7': 0.5969026041820608,
# '7,3': 0.6283185307179586,
# '3,6': 0.6597344572538566,
# '6,1': 0.6911503837897546,
# '1,5': 0.6911503837897546}
						</code></pre>
						<p>Note: transition time $\approx 2\pi/k$.</p>
					</section>
					<section>
						<p>Times series prediction:</p>
						<pre class="python"><code>
def simulate(centroids: np.ndarray, transition_times: Dict[str, float],
             start_id: int, end_time) -> Tuple[np.ndarray, np.ndarray]:
    visited_centroids, time = [start_id], [0.0]
    while time[-1] < end_time:
        visited_centroids.append(get_next_cluster(visited_centroids[-1], transition_times.keys()))
        transition = "{:d},{:d}".format(*visited_centroids[-2:])
        time.append(time[-1] + transition_times[transition])
        if time[-1] > end_time:
            break
    visited = np.zeros((len(time), centroids.shape[1]))
    for i, ci in enumerate(visited_centroids):
        visited[i] = centroids[ci]
    return visited, np.array(time)
						</code></pre>
					</section>
					<section>
						<img src="images/periodic_dataset_simulated.svg" alt="probes" width="75%" style="background:none; border:none; box-shadow:none;">
						<p>Time series prediction of network with 10 clusters.</p>
					</section>
					<section>
						<img src="images/periodic_dataset_clustering_refined.svg" alt="probes" width="50%" style="background:none; border:none; box-shadow:none;">
						<p>Clustering of time series data using 20 centroids.</p>
					</section>
					<section>
						<p>Problem: how to deal with uncertainty?</p>
						<pre class="python"><code>
transition_times = compute_transition_time(cluster_ids, 2.0*np.pi/100)
transition_times
# output
# {'18,13': 0.25132741228718347,
# ...
#  '10,4': 0.06283185307179587,
#  '10,8': 0.3141592653589793,
#  '8,18': 0.21991148575128555}
						</code></pre>
					</section>
					<section>
						<p>Modeling of transition probabilities:</p>
						<ol>
							<li>find all possible transitions</li>
							<li>count occurrences of transitions</li>
							<li>compute transition probability based on count</li>
						</ol>
						<p>Prediction: <b>sample next cluster randomly</b></p>
					</section>
					<section>
						<p>Transition probability example:</p>
						<pre class="python"><code>
# Possible next clusters:  {... '4': [10, 10], '10': [4, 8], '8': [18]})
# transition probabilities
# {'18': array([[13.,  1.]]),
# ...
# '4': array([[10.,  1.]]),
# '10': array([[4. , 0.5],
#              [8. , 0.5]]),
# '8': array([[18.,  1.]])})
						</code></pre>
					</section>
					<section>
						<img src="images/periodic_dataset_simulated_refined.svg" alt="probes" width="75%" style="background:none; border:none; box-shadow:none;">
						<p>Time series prediction of network with 20 clusters.</p>
					</section>
				</section>

				<section>
					<section>
						<h2>Dealing with high-dimensional data</h2>
					</section>
					<section>
						<p>General workflow:</p>
						<ol>
							<li>encode:<br> map full state to reduced state</li>
							<li>evolve:<br> predict time evolution of reduced state</li>
							<li>decode:<br> reconstruct full state</li>
						</ol>
					</section>
					<section>
						<p>ROM based on CNM and POD</p>
						<ol>
							<li>encode:<br> project time series data onto POD modes</li>
							<li>evolve:<br> predict mode coefficients with CNM</li>
							<li>decode:<br> inner product of modes and coefficients</li>
						</ol>
					</section>
					<section>
						<p>Encoding with POD/SVD:</p>
						<pre class="python"><code>
def encode(data_matrix: pt.Tensor, rank: int) -> Tuple[pt.Tensor, pt.Tensor]:
    U, s, VH = pt.linalg.svd(data_matrix, full_matrices=False)
    return U[:, :rank], pt.diag(s[:rank]) @ VH[:rank, :]

# the data matrix consists of 241 snapshots of the vorticity field
modes, coeff = encode(data_matrix, rank=20)
print("Mode matrix shape: ", modes.shape)
print("Coeff. matrix shape: ", coeff.shape)
						</code></pre>
					</section>
					<section>
						<p>Shape of mode matrix if $m$ points per snapshot, $n$ snapshots, and rank $r$ truncation?</p>
						<ol>
							<li style="list-style-type: upper-latin;">$m\times m$</li>
							<li style="list-style-type: upper-latin;">$m\times n$</li>
							<li style="list-style-type: upper-latin;">$m\times r$</li>
							<li style="list-style-type: upper-latin;">$n\times r$</li>
							<li style="list-style-type: upper-latin;">$r\times m$</li>
						</ol>
					</section>
					<section>
						<p>Shape of mode coefficient matrix if $m$ points per snapshot, $n$ snapshots, and rank $r$ truncation?</p>
						<ol>
							<li style="list-style-type: upper-latin;">$n\times n$</li>
							<li style="list-style-type: upper-latin;">$n\times m$</li>
							<li style="list-style-type: upper-latin;">$m\times n$</li>
							<li style="list-style-type: upper-latin;">$n\times r$</li>
							<li style="list-style-type: upper-latin;">$r\times n$</li>
						</ol>
					</section>
					<section>
						<p>Clustering of the mode coefficients:</p>
						<pre class="python"><code>
clustering = KMeans(n_clusters=40, random_state=0)
clustering.fit(coeff.T.numpy())
centroids = clustering.cluster_centers_
print(centroids.shape)
						</code></pre>
					</section>
					<section>
						<p>Of how many entries/elements consists a single centroid?</p>
						<ol>
							<li style="list-style-type: upper-latin;">$n$</li>
							<li style="list-style-type: upper-latin;">$m$</li>
							<li style="list-style-type: upper-latin;">$r$</li>
							<li style="list-style-type: upper-latin;">$40$</li>
						</ol>
					</section>
					<section>
						<p>Time evolution with CNM</p>
						<pre class="python"><code>
# transition times
transition_times = compute_transition_time(clustering.labels_, dt)
# transition probabilities
centroid_sequence = remove_sequential_duplicates(clustering.labels_)
transition_probs = compute_transition_probabilities(centroid_sequence)
# time evolution
prediction, times_sim = simulate_probablisticly(
	centroids, transition_probs, transition_times, clustering.labels_[0], 10)
						</code></pre>
					</section>
					<section>
						<img src="images/cylinder_coeff_prediction_cnm.svg" alt="probes" width="80%" style="background:none; border:none; box-shadow:none;">
						<p>CNM prediction of POD mode coefficients.</p>
					</section>
					<section>
						<p>Reconstruction of the vorticity field:</p>
						<pre class="python"><code>
def decode(modes: pt.Tensor, coeff: pt.Tensor) -> pt.Tensor:
    """Compute inner product of POD modes and coefficients.
	"""
    return modes @ coeff

# reconstruct full state
reconstruction = decode(modes, pt.from_numpy(prediction).T.type(pt.float32))
						</code></pre>
					</section>

					<section>
						<img src="images/cylinder_full_prediction_cnm.svg" alt="probes" width="80%" style="background:none; border:none; box-shadow:none;">
						<p>Reconstructed CNM prediction of vorticity field.</p>
					</section>
				</section>
-->

			</div>

		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/zoom/zoom.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/search/search.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/math/math.js"></script>
		<script src="plugin/chalkboard/plugin.js"></script>
		<script src="plugin/customcontrols/plugin.js"></script>
		<script>

			// Also available as an ES module, see:
			// https://revealjs.com/initialization/
			Reveal.initialize({
			hash: true,
			progress: true,
			slideNumber: "c",
			chalkboard: {
				boardmarkerWidth: 4,
				chalkWidth: 4,
				chalkEffect: 0.01,
				theme: "chalkboard",
				background: ['rgba(127,127,127,.1)', path + 'img/blackboard.png'],
				grid: { color: 'rgb(50,50,10,0.5)', distance: 80, width: 2 },
				eraser: { src: path + 'img/sponge.png', radius: 20 },
				boardmarkers: [
					{ color: 'rgba(255,255,255,1)', cursor: 'url(' + path + 'img/boardmarker-black.png), auto' },
					{ color: 'rgba(30,144,255, 1)', cursor: 'url(' + path + 'img/boardmarker-blue.png), auto' },
					{ color: 'rgba(220,20,60,1)', cursor: 'url(' + path + 'img/boardmarker-red.png), auto' },
					{ color: 'rgba(50,205,50,1)', cursor: 'url(' + path + 'img/boardmarker-green.png), auto' },
					{ color: 'rgba(255,140,0,1)', cursor: 'url(' + path + 'img/boardmarker-orange.png), auto' },
					{ color: 'rgba(150,0,20150,1)', cursor: 'url(' + path + 'img/boardmarker-purple.png), auto' },
					{ color: 'rgba(255,220,0,1)', cursor: 'url(' + path + 'img/boardmarker-yellow.png), auto' }
				],
				chalks: [
					{ color: 'rgba(255,255,255, 0.5)', cursor: 'url(' + path + 'img/chalk-white.png), auto' },
					{ color: 'rgba(96, 154, 244, 0.5)', cursor: 'url(' + path + 'img/chalk-blue.png), auto' },
					{ color: 'rgba(237, 20, 28, 0.5)', cursor: 'url(' + path + 'img/chalk-red.png), auto' },
					{ color: 'rgba(20, 237, 28, 0.5)', cursor: 'url(' + path + 'img/chalk-green.png), auto' },
					{ color: 'rgba(220, 133, 41, 0.5)', cursor: 'url(' + path + 'img/chalk-orange.png), auto' },
					{ color: 'rgba(220,0,220,0.5)', cursor: 'url(' + path + 'img/chalk-purple.png), auto' },
					{ color: 'rgba(255,220,0,0.5)', cursor: 'url(' + path + 'img/chalk-yellow.png), auto' }
				],
			},
			customcontrols: {
				controls: [
					{
						icon: '<i class="fa fa-pencil-square" style="font-size:36px"></i>',
						title: 'Toggle chalkboard (B)',
						action: 'RevealChalkboard.toggleChalkboard();'
					},
					{
						icon: '<i class="fa fa-pencil" style="font-size:36px"></i>',
						title: 'Toggle notes canvas (C)',
						action: 'RevealChalkboard.toggleNotesCanvas();'
					}
				]
			},

			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [RevealZoom, RevealNotes, RevealSearch, RevealMarkdown, RevealHighlight, RevealMath, RevealChalkboard, RevealCustomControls]
		});

		</script>
		<style type="text/css">
			.reveal .slide-number {
				font-size: 1em;
				color: #42affa;
				right: auto;
				width: 1.5em;
				height: 1.5em;
				background-color: #191919;
				display: grid !important;
				place-items: center;
			}
		</style>

	</body>
</html>
